{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59094,"databundleVersionId":7010844,"sourceType":"competition"},{"sourceId":6567471,"sourceType":"datasetVersion","datasetId":3755626},{"sourceId":6792212,"sourceType":"datasetVersion","datasetId":3907473}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Open problems - Single cell perturbations\n## Combining gene expression modeling + Limma\n#### by Antoine Passemiers\n\n---\n\nIn this notebook, we implemented a neural network to estimate gene expression and use Limma to convert predictions to DE values. We prealably saved the normalized read counts, as produced by the `voom` R function from the `limma` package. ","metadata":{}},{"cell_type":"markdown","source":"<h1 id=\"dependencies\">Install dependencies and import libraries</h1> <a class=\"anchor\" id=\"dependencies\"></a>\n\nLet's first install R. It is a very time-consuming process, so please be patient.","metadata":{}},{"cell_type":"code","source":"import subprocess\nsubprocess.run(\n    'conda install --repodata-fn repodata.json -c conda-forge r-base',\n    shell=True,\n    stdout=subprocess.DEVNULL,\n    stderr=subprocess.DEVNULL\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T07:35:54.874532Z","iopub.execute_input":"2023-12-12T07:35:54.875001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rpy2 category_encoders","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:41:27.664490Z","iopub.execute_input":"2023-12-11T16:41:27.664941Z","iopub.status.idle":"2023-12-11T16:42:07.668062Z","shell.execute_reply.started":"2023-12-11T16:41:27.664905Z","shell.execute_reply":"2023-12-11T16:42:07.666569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport warnings\nfrom typing import Dict, List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, RobustScaler\nfrom sklearn.decomposition import KernelPCA\nimport pandas as pd\nimport tqdm\nimport torch\n\nimport category_encoders\nimport rpy2\nimport rpy2.robjects as robjects\nfrom rpy2.robjects import pandas2ri\nfrom rpy2.robjects.packages import SignatureTranslatedAnonymousPackage\nfrom rpy2.robjects.packages import importr\nimport rpy2.rinterface as ri\npandas2ri.activate()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:42:07.671573Z","iopub.execute_input":"2023-12-11T16:42:07.672122Z","iopub.status.idle":"2023-12-11T16:42:12.946082Z","shell.execute_reply.started":"2023-12-11T16:42:07.672063Z","shell.execute_reply":"2023-12-11T16:42:12.944911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 0xCAFE\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.use_deterministic_algorithms(True)\ntorch.set_num_threads(1)\ntorch.manual_seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:42:30.710876Z","iopub.execute_input":"2023-12-11T16:42:30.711314Z","iopub.status.idle":"2023-12-11T16:42:30.729461Z","shell.execute_reply.started":"2023-12-11T16:42:30.711281Z","shell.execute_reply":"2023-12-11T16:42:30.728028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load data","metadata":{}},{"cell_type":"code","source":"#ROOT = os.path.dirname(os.path.abspath(__file__))\nROOT = '..'\nDATA_FOLDER = os.path.join(ROOT, 'data')\nDATA_FOLDER = '/kaggle/input/open-problems-single-cell-perturbations/'\n# EXTERNAL_DATA_FOLDER = os.path.join(ROOT, 'external-data')\nEXTERNAL_DATA_FOLDER = '/kaggle/input/op2-expression-data/'\n\nINPUT_CELL_TYPES = ['NK cells', 'T cells CD4+', 'T regulatory cells']\n#INPUT_CELL_TYPES = ['NK cells', 'T cells CD4+', 'T cells CD8+', 'T regulatory cells']\nOUTPUT_CELL_TYPES = ['Myeloid cells', 'B cells']\nCELL_TYPES = INPUT_CELL_TYPES + OUTPUT_CELL_TYPES\n\n\ndf = pd.read_parquet(os.path.join(DATA_FOLDER, 'de_train.parquet'))\n\ncell_types = df['cell_type']\nsm_names = df['sm_name']\nsm_lincs_ids = df['sm_lincs_id']\nall_smiles = df['SMILES']\nis_control = df['control']\n\ngroups = LabelEncoder().fit_transform(sm_names)\n\nfor col_name in ['cell_type', 'sm_name', 'sm_lincs_id', 'SMILES', 'control']:\n    df.drop(col_name, axis=1, inplace=True)\ndata = df.to_numpy(dtype=float)\n\nunique_smiles = list(set(all_smiles))\nsmiles_dict = {smiles: i for i, smiles in enumerate(unique_smiles)}\nunique_cell_types = list(set(cell_types))\nunique_sm_names = list(set(sm_names))\ncell_type_dict = {cell_type: i for i, cell_type in enumerate(unique_cell_types)}\nsm_name_dict = {sm_name: i for i, sm_name in enumerate(unique_sm_names)}\nsm_name_dict['Dimethyl Sulfoxide'] = len(sm_name_dict)\ngene_names = list(df.columns)\ngene_name_dict = {gene_name: i for i, gene_name in enumerate(gene_names)}\nsm_name_to_smiles = {sm_name: smiles for sm_name, smiles in zip(sm_names, all_smiles)}\ndonor_dict = {f'donor_{i}': i for i in range(3)}\nplate_name_dict = {f'plate_{i}': i for i in range(6)}\nrow_dict = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7}","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:42:32.697443Z","iopub.execute_input":"2023-12-11T16:42:32.698135Z","iopub.status.idle":"2023-12-11T16:42:35.973953Z","shell.execute_reply.started":"2023-12-11T16:42:32.698101Z","shell.execute_reply":"2023-12-11T16:42:35.972369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NN(torch.nn.Module):\n    \n    def __init__(self, n_out: int):\n        torch.nn.Module.__init__(self)\n        \n        self.n_in_channels: int = 5\n        self.n_in: int = n_out\n        self.n_out: int = n_out\n        self.encoder = torch.nn.Sequential(\n            \n            torch.nn.Linear(self.n_in * self.n_in_channels, 128),\n            torch.nn.PReLU(128),\n            \n            torch.nn.Linear(128, 256),\n            #torch.nn.BatchNorm1d(256),\n            torch.nn.PReLU(256),\n            #torch.nn.Dropout(0.2, inplace=False),\n            \n            torch.nn.Linear(256, 256),\n            torch.nn.PReLU(256),\n            #torch.nn.Dropout(0.2, inplace=False),\n            \n            torch.nn.Linear(256, 128),\n            torch.nn.PReLU(128),\n            #torch.nn.BatchNorm1d(128),\n            #torch.nn.Dropout(0.2, inplace=False),\n            \n            torch.nn.Linear(128, 128),\n            torch.nn.PReLU(128),\n            \n            torch.nn.Linear(128, self.n_out * 5),\n        )\n        \n        self.gene_regressor = torch.nn.Sequential(\n            \n            torch.nn.Linear(10, 128),\n            torch.nn.PReLU(128),\n            \n            torch.nn.Linear(128, 128),\n            torch.nn.PReLU(128),\n            \n            torch.nn.Linear(128, 1),\n        )\n        \n        self.weight_regressor = torch.nn.Sequential(\n            \n            torch.nn.Linear(10, 64),\n            torch.nn.PReLU(64),\n            \n            torch.nn.Linear(64, 64),\n            torch.nn.PReLU(64),\n            \n            torch.nn.Linear(64, 1),\n            torch.nn.ReLU()\n        )\n\n        def init_weights(m):\n            if isinstance(m, torch.nn.Linear):\n                torch.nn.init.xavier_uniform_(m.weight)\n                m.bias.data.fill_(0.001)\n        self.encoder.apply(init_weights)\n        self.gene_regressor.apply(init_weights)\n        self.weight_regressor.apply(init_weights)\n        \n    def forward(self, X: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        encoded = self.encoder(X.reshape(len(X), -1)).reshape(len(X), -1, 5)\n        encoded = torch.cat((encoded, X), dim=2)\n        Y = self.gene_regressor(encoded.reshape(-1, encoded.size()[2])).reshape(len(X), -1)\n        W = self.weight_regressor(encoded.reshape(-1, encoded.size()[2])).reshape(len(X), -1)\n        return Y, W\n    \n    def predict(self, X: np.ndarray, n_batches: int = 100) -> Tuple[np.ndarray, np.ndarray]:\n        X = torch.FloatTensor(X)\n        Y_pred_s, W_pred_s = [], []\n        for idx in np.array_split(np.arange(len(X)), n_batches):\n            Y_pred, W_pred = self.forward(X[idx, :])\n            Y_pred_s.append(Y_pred.cpu().data.numpy())\n            W_pred_s.append(W_pred.cpu().data.numpy())\n        Y_pred = np.concatenate(Y_pred_s, axis=0)\n        W_pred = np.concatenate(W_pred_s, axis=0)\n        return Y_pred, W_pred","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:42:35.976355Z","iopub.execute_input":"2023-12-11T16:42:35.976917Z","iopub.status.idle":"2023-12-11T16:42:36.004498Z","shell.execute_reply.started":"2023-12-11T16:42:35.976864Z","shell.execute_reply":"2023-12-11T16:42:36.002937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Target encoding","metadata":{}},{"cell_type":"code","source":"df_s, y_s, weights_s = [], [], []\nfor cell_type in CELL_TYPES:\n    cell_type = cell_type.replace('+', '')\n    meta_df = pd.read_csv(os.path.join(EXTERNAL_DATA_FOLDER, f'meta-{cell_type}.tsv'), delimiter='\\t', header='infer')\n    r_data = np.load(os.path.join(EXTERNAL_DATA_FOLDER, f'{cell_type}.npz'))\n    y = r_data['expression']  # Gene expression values, of shape (n_samples, n_genes)\n    weights = r_data['weights']  # Inverse variance weights, of shape (n_samples, n_genes)\n    df_s.append(meta_df)\n    y_s.append(y)\n    weights_s.append(weights)\nmeta_df = pd.concat(df_s, axis=0)\ny = np.concatenate(y_s, axis=0)\nweights = np.concatenate(weights_s, axis=0)\n\nmeta_idx = {key: i for i, key in enumerate(zip(*[meta_df[col_name] for col_name in ['sm_name', 'cell_type', 'donor_id', 'plate_name', 'row']]))}","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:42:36.006076Z","iopub.execute_input":"2023-12-11T16:42:36.006517Z","iopub.status.idle":"2023-12-11T16:42:39.074705Z","shell.execute_reply.started":"2023-12-11T16:42:36.006484Z","shell.execute_reply":"2023-12-11T16:42:39.073419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiOutputTargetEncoder:\n    \"\"\"Multi-output target encoder.\n    \n    Each input (categorical) feature will be encoded based on each (continuous) output variable.\n    \n    Attributes:\n        encoders: List of encoders, of shape `n_genes`.\n    \"\"\"\n    \n    def __init__(self):\n        self.encoders: List[category_encoders.leave_one_out.LeaveOneOutEncoder] = []\n        \n    @staticmethod\n    def new_encoder() -> category_encoders.leave_one_out.LeaveOneOutEncoder:\n        \"\"\"Instantiates a new gene-specific target encoder.\n        \n        Returns:\n            New instance of a target encoder.\n        \"\"\"\n        return category_encoders.leave_one_out.LeaveOneOutEncoder(return_df=False)\n    \n    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n        \"\"\"Fit the encoders for each input feature and output variable.\n        \n        Args:\n            X: Array of shape `(n, n_features)` containing categories as strings or integers.\n                Typical, `n_features` is equal to 2 (cell type + compound).\n            y: Array of shape `(n, n_genes)` containing the DE values for all the genes.\n        \"\"\"\n        self.encoders = []\n        for j in tqdm.tqdm(range(y.shape[1]), desc='fit LOO encoders'):\n            self.encoders.append(MultiOutputTargetEncoder.new_encoder())\n            self.encoders[-1].fit(X, y[:, j])\n    \n    def transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Encodes the categories. Assumes the encoders have already been fitted.\n        \n        Args:\n            X: Array of shape `(n, n_features)` containing categories as strings or integers.\n        \n        Returns:\n            Array of shape `(n, n_genes, n_features)` containing the encoding of each input\n                feature with respect to each output variable.\n        \"\"\"\n        Z = []\n        for encoder in tqdm.tqdm(self.encoders, desc='transform LOO encoders'):\n            y_hat = encoder.transform(X)\n            Z.append(y_hat)\n        Z = np.asarray(Z)\n        return np.transpose(Z, (1, 0, 2))","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:42:39.077332Z","iopub.execute_input":"2023-12-11T16:42:39.077796Z","iopub.status.idle":"2023-12-11T16:42:39.092685Z","shell.execute_reply.started":"2023-12-11T16:42:39.077752Z","shell.execute_reply":"2023-12-11T16:42:39.091418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = RobustScaler()\ny_scaled = scaler.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:42:39.094361Z","iopub.execute_input":"2023-12-11T16:42:39.095365Z","iopub.status.idle":"2023-12-11T16:42:45.017775Z","shell.execute_reply.started":"2023-12-11T16:42:39.095320Z","shell.execute_reply":"2023-12-11T16:42:45.016331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = MultiOutputTargetEncoder()\nencoder.fit(meta_df[['sm_name', 'donor_id', 'plate_name', 'row', 'cell_type']], y_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:42:45.019945Z","iopub.execute_input":"2023-12-11T16:42:45.020319Z","iopub.status.idle":"2023-12-11T16:54:47.857659Z","shell.execute_reply.started":"2023-12-11T16:42:45.020288Z","shell.execute_reply":"2023-12-11T16:54:47.856474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = encoder.transform(meta_df[['sm_name', 'donor_id', 'plate_name', 'row', 'cell_type']])\nX = torch.FloatTensor(X)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T17:05:19.698789Z","iopub.execute_input":"2023-12-11T17:05:19.699195Z","iopub.status.idle":"2023-12-11T17:11:11.277382Z","shell.execute_reply.started":"2023-12-11T17:05:19.699164Z","shell.execute_reply":"2023-12-11T17:11:11.275662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(idx_train):\n    idx_train = np.copy(idx_train)\n    model = NN(len(gene_names))\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, eps=1e-7)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='min', factor=0.9, patience=10, verbose=False, threshold=0.0001,\n            threshold_mode='rel', cooldown=2, min_lr=1e-5, eps=1e-08)\n    pbar = tqdm.tqdm(range(10))\n    for epoch in pbar:\n        total_loss, baseline_total_loss = 0, 0\n        np.random.shuffle(idx_train)\n        for it_idx in np.array_split(idx_train, 500):\n            optimizer.zero_grad()\n            \n            Y = torch.FloatTensor(y_scaled[it_idx, :])\n            W = torch.FloatTensor(weights[it_idx, :])\n            W = torch.clamp(W, 0, 100)\n            #Y = Y + 0.5 * torch.randn(*Y.size())\n            Y_pred, W_pred = model.forward(X[it_idx])\n            mrrmse = torch.mean(torch.sqrt(torch.mean(torch.square(Y - Y_pred), dim=1)))\n            baseline_mrrmse = torch.mean(torch.sqrt(torch.mean(torch.square(Y - torch.mean(Y, dim=0).unsqueeze(0)), dim=1)))\n            loss = torch.mean(torch.square(Y - Y_pred))\n            loss = loss + 0.5 * torch.mean(torch.square((W - W_pred) / 100))\n            loss.backward()\n            optimizer.step()\n            total_loss += mrrmse.item()\n            baseline_total_loss += baseline_mrrmse.item()\n        rel_error = total_loss / baseline_total_loss\n        scheduler.step(rel_error)    \n        pbar.set_description(f'{rel_error:.3f}')\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-11T17:11:33.916579Z","iopub.execute_input":"2023-12-11T17:11:33.919024Z","iopub.status.idle":"2023-12-11T17:11:33.939074Z","shell.execute_reply.started":"2023-12-11T17:11:33.918935Z","shell.execute_reply":"2023-12-11T17:11:33.937325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\nrel_errors = []\nkf = KFold(n_splits=10, random_state=SEED, shuffle=True)\nfor i, (train_index, test_index) in enumerate(kf.split(y)):\n    \n    # Train model\n    model = train(train_index)\n    model.eval()\n    models.append(model)\n\n    # Predict on held-out samples\n    Y = torch.FloatTensor(y_scaled[test_index, :])\n    Y_pred, W_pred = model.forward(X[test_index])\n    \n    plt.scatter(Y[0, :].cpu().data.numpy(), Y_pred[0, :].cpu().data.numpy(), alpha=0.1)\n    plt.show()\n    \n    #mrrmse = torch.mean(torch.abs(Y - Y_pred)).item()\n    #baseline_mrrmse = torch.mean(torch.abs(Y - torch.mean(Y, dim=0).unsqueeze(0))).item()\n    mrrmse = torch.mean(torch.sqrt(torch.mean(torch.square(Y - Y_pred), dim=1))).item()\n    baseline_mrrmse = torch.mean(torch.sqrt(torch.mean(torch.square(Y - torch.mean(Y, dim=0).unsqueeze(0)), dim=1))).item()\n    rel_errors.append(mrrmse / baseline_mrrmse)\n    print(rel_errors[-1])\n    \nprint(f'Average relative error: {np.mean(rel_errors)}')\n\n# Average relative error: 0.6952671768202316\n# Average relative error: 0.6932183754092522","metadata":{"execution":{"iopub.status.busy":"2023-12-11T17:11:34.961381Z","iopub.execute_input":"2023-12-11T17:11:34.961854Z","iopub.status.idle":"2023-12-11T18:01:00.826564Z","shell.execute_reply.started":"2023-12-11T17:11:34.961817Z","shell.execute_reply":"2023-12-11T18:01:00.825391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train(np.arange(len(y)))","metadata":{"execution":{"iopub.status.busy":"2023-12-11T18:12:47.282569Z","iopub.execute_input":"2023-12-11T18:12:47.283191Z","iopub.status.idle":"2023-12-11T19:15:05.911508Z","shell.execute_reply.started":"2023-12-11T18:12:47.283142Z","shell.execute_reply":"2023-12-11T19:15:05.908615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## Make submission\n\n---","metadata":{}},{"cell_type":"code","source":"class Limma:\n    \n    r_utils = importr('utils')\n    r_utils.chooseCRANmirror(ind=1)\n    r_utils.install_packages('BiocManager')\n    r_biocmanager = importr('BiocManager')\n    r_biocmanager.install('limma')\n    r_limma = importr('limma')\n    r_code = \"\"\"\n    library(limma)\n    create_elist <- function(expression, weights) {\n        E <- new(\"EList\")\n        E$E <- expression\n        E$weights <- weights\n        return(E)\n    }\n    make_design <- function(df) {\n        for (column in colnames(df)){\n            assign(column, df[[column]])\n        }\n        mm <- model.matrix(eval(parse(text=\"~0+sm_name+donor_id+plate_name+row\")))\n        return(mm)\n    }\n    \"\"\"\n    r_package = SignatureTranslatedAnonymousPackage(r_code, 'custom')\n    \n    @staticmethod\n    def run(df: pd.DataFrame, y: np.ndarray, weights: np.ndarray) -> np.ndarray:\n        \"\"\"Run Limma to obtain the DE values.\n        \n        Args:\n            df: Pandas dataframe containing 4 columns, namely \"compound\", \"donor_id\", \"plate_name\" and \"row\".\n            y: Gene expression values, a matrix of shape (n_samples, n_genes). The number of rows in `df` is \n                equal to the number of rows in `y`.\n            weights: Inverse variance weights, a matrix of shape (n_samples, n_genes). The dimensions of `weights`\n                should match the dimensions of `y`.\n        \n        Returns:\n            A matrix of shape (n_contrasts, n_genes), where `n_contrasts` is the number of compounds after\n                excluding \"Dimethyl Sulfoxide\".\n        \"\"\"\n        \n        # Create design matrix and targets\n        X = Limma.r_package.make_design(df)\n        yw = Limma.r_package.create_elist(\n            rpy2.robjects.r.matrix(y.T, nrow=y.T.shape[0], ncol=y.T.shape[1]),\n            rpy2.robjects.r.matrix(weights.T, nrow=weights.T.shape[0], ncol=weights.T.shape[1])\n        )\n        print(f'Design matrix: {X.shape}')\n\n        # Build contrast matrix, also keep track in `contrast_dict` of where each compound\n        # is located in design matrix\n        sm_name_to_col_idx = {}\n        for p in range(len(X)):\n            sm_name = df.loc[p, 'sm_name']\n            j = np.where(X[p, :])[0][0]\n            if sm_name not in sm_name_to_col_idx:\n                sm_name_to_col_idx[sm_name] = j\n            else:\n                assert sm_name_to_col_idx[sm_name] == j\n        C = np.zeros((X.shape[1], len(sm_name_to_col_idx) - 1))\n        p = 0\n        contrast_dict = {}\n        unique_sm_names = set(list(df['sm_name']))\n        for sm_name in unique_sm_names:\n            if sm_name == 'Dimethyl Sulfoxide':\n                continue\n            if sm_name not in sm_name_to_col_idx:\n                continue\n            i = sm_name_to_col_idx[sm_name]\n            j = sm_name_to_col_idx['Dimethyl Sulfoxide']\n            C[i, p] = 1\n            C[j, p] = -1\n            contrast_dict[sm_name] = p\n            p += 1\n        print(f'Contrast matrix: {C.shape}')\n        \n        # Fit linear models\n        print(f'Running linear models.')\n        fit = Limma.r_limma.lmFit(\n            yw,\n            rpy2.robjects.r.matrix(X, nrow=X.shape[0], ncol=X.shape[1])\n        )\n        \n        # Replace covariates by their contrasts\n        print(f'Computing contrasts.')\n        fit = Limma.r_limma.contrasts_fit(\n            fit,\n            rpy2.robjects.r.matrix(C, nrow=C.shape[0], ncol=C.shape[1])\n        )\n        \n        # Use an empirical Bayes approach to squeeze the residual variances\n        print(f'Running empirical Bayes.')\n        fit = Limma.r_limma.eBayes(fit)\n        \n        # Compute DE values\n        de = -np.sign(fit.rx2('t')) * np.log10(fit.rx2('p.value'))\n        print(f'Done.')\n        \n        return de, contrast_dict","metadata":{"execution":{"iopub.status.busy":"2023-12-11T19:23:05.400612Z","iopub.execute_input":"2023-12-11T19:23:05.401193Z","iopub.status.idle":"2023-12-11T19:23:42.439490Z","shell.execute_reply.started":"2023-12-11T19:23:05.401137Z","shell.execute_reply":"2023-12-11T19:23:42.437903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data(cell_type, model):\n    Y, W = [], []\n    compounds, plates, donors, rows, cell_types = [], [], [], [], []\n    for plate in [f'plate_{i}' for i in range(6)]:\n        for donor in [f'donor_{i}' for i in range(3)]:\n            for row in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']:\n                for compound in sm_name_dict.keys():\n                    \"\"\"\n                    if (compound, cell_type, donor, plate, row) in meta_idx:\n                        i = meta_idx[(compound, cell_type, donor, plate, row)]\n                        Y.append(y[i, :])\n                        W.append(weights[i, :])\n                        ok = True\n                    \"\"\"\n                    if random.random() < 0.1:\n                        Y.append(np.full(len(gene_names), np.nan))\n                        W.append(np.full(len(gene_names), np.nan))\n                        ok = True\n                    else:\n                        ok = False\n                    if ok:\n                        compounds.append(compound)\n                        plates.append(plate)\n                        donors.append(donor)\n                        rows.append(row)\n                        cell_types.append(cell_type)\n    Y = np.asarray(Y)\n    W = np.asarray(W)\n                        \n    df = pd.DataFrame({\n        'sm_name': np.asarray(compounds),\n        'donor_id': np.asarray(donors),\n        'plate_name': np.asarray(plates),\n        'row': np.asarray(rows),\n        'cell_type': np.asarray(cell_types)\n    })\n    print(df)\n    X = torch.FloatTensor(encoder.transform(df))\n    Y_pred, W_pred = model.predict(X)\n    assert Y.shape == Y_pred.shape\n    assert W.shape == W_pred.shape\n    \n    Y_pred = scaler.inverse_transform(Y_pred)\n    \n    print(len(df), Y.shape, W.shape)\n    \n    return df, Y_pred, W_pred\n    \n\nall_de_pred = {}\nfor cell_type in OUTPUT_CELL_TYPES:\n    df, Y, W = prepare_data(cell_type, model)\n    de, contrast_dict = Limma.run(df, Y, W)\n    de[np.isnan(de)] = 0\n    de[np.isinf(de)] = 0\n    all_de_pred[cell_type] = (de, contrast_dict)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T19:25:25.659058Z","iopub.execute_input":"2023-12-11T19:25:25.659556Z","iopub.status.idle":"2023-12-11T20:08:13.742555Z","shell.execute_reply.started":"2023-12-11T19:25:25.659520Z","shell.execute_reply":"2023-12-11T20:08:13.739940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load ID mapping\nid_map = []\nwith open(os.path.join(DATA_FOLDER, 'id_map.csv'), 'r') as f:\n    lines = f.readlines()[1:]\n    for line in lines:\n        id_map.append(line.rstrip().split(','))\n        assert len(id_map[-1]) == 3\n\n# Make submission\nwith open('submission.csv', 'w') as f:\n    f.write(f'id,{\",\".join(gene_names)}\\n')\n    for id_, cell_type, sm_name in tqdm.tqdm(id_map, desc='Submission'):\n\n        # Predict on test sample\n        de, contrast_dict = all_de_pred[cell_type]\n        i = contrast_dict[sm_name]\n        y_hat = np.nan_to_num(de[:, i])\n        y_hat = np.clip(y_hat, -100, 100)\n\n        # Write predictions in output file\n        values = [f'{x:.5f}' for x in y_hat]\n        f.write(f'{id_},{\",\".join(values)}\\n')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T00:41:12.248265Z","iopub.execute_input":"2023-11-23T00:41:12.248728Z","iopub.status.idle":"2023-11-23T00:41:16.502457Z","shell.execute_reply.started":"2023-11-23T00:41:12.248690Z","shell.execute_reply":"2023-11-23T00:41:16.501266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}